{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import twokenize.twokenize as tokenizer\n",
    "\n",
    "from pipelines.helpers import ItemGetter\n",
    "\n",
    "def make_classifier():\n",
    "    clf = Pipeline([\n",
    "        (\"getter\", ItemGetter(\"text\")),\n",
    "        (\"tfidf\", TfidfVectorizer()),\n",
    "        (\"clf\", LogisticRegression())])\n",
    "\n",
    "    clf_params = {\n",
    "        'clf__C': 200,\n",
    "        'clf__dual': False,\n",
    "        'clf__max_iter': 100,\n",
    "        'clf__multi_class': 'ovr',\n",
    "        'clf__penalty': 'l2',\n",
    "        'tfidf__tokenizer':tokenizer.tokenize,\n",
    "        'tfidf__ngram_range':(1, 3),\n",
    "        'tfidf__max_features':200000\n",
    "    }\n",
    "\n",
    "    clf.set_params(**clf_params)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf_alc = make_classifier()\n",
    "clf_fpa = make_classifier()\n",
    "clf_fpl = make_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf_fpa = make_classifier()\n",
    "clf_fpl = make_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 29.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from data import DataAccess, LabelGetter\n",
    "\n",
    "X = DataAccess.get_as_dataframe()\n",
    "L = LabelGetter(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('getter', ItemGetter(key='text')), ('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=200000, min_df=1,\n",
       "        ngram_range=(1, 3), norm='l2',...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "clf_alc.fit(*L.get_alcohol())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = L.get_alcohol()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.33, random_state=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('getter', ItemGetter(key='text')), ('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=200000, min_df=1,\n",
       "        ngram_range=(1, 3), norm='l2',...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_alc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from classification.reporting import ClassificationReporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reporting = ClassificationReporting(clf_alc, X_train, X_test, y_train, y_test, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tom Work\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:976: DeprecationWarning: From version 0.18, binary input will not be handled specially when using averaged precision/recall/F-score. Please use average='binary' to report only the positive class performance.\n",
      "  'positive class performance.', DeprecationWarning)\n",
      "C:\\Users\\Tom Work\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:976: DeprecationWarning: From version 0.18, binary input will not be handled specially when using averaged precision/recall/F-score. Please use average='binary' to report only the positive class performance.\n",
      "  'positive class performance.', DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Results\n",
      "~~~~~~~~~~~~~~~~\n",
      "confusion_matrix\n",
      "[[3632, 0], [0, 6853]]\n",
      "\n",
      "\n",
      "classification_report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00      3632\n",
      "          1       1.00      1.00      1.00      6853\n",
      "\n",
      "avg / total       1.00      1.00      1.00     10485\n",
      "\n",
      "\n",
      "\n",
      "f1_score\n",
      "1.0\n",
      "\n",
      "\n",
      "accuracy_score\n",
      "1.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Testing Results Results\n",
      "~~~~~~~~~~~~~~~~~~~~~~~\n",
      "confusion_matrix\n",
      "[[888, 835], [161, 3281]]\n",
      "\n",
      "\n",
      "classification_report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.85      0.52      0.64      1723\n",
      "          1       0.80      0.95      0.87      3442\n",
      "\n",
      "avg / total       0.81      0.81      0.79      5165\n",
      "\n",
      "\n",
      "\n",
      "f1_score\n",
      "0.8682191055834877\n",
      "\n",
      "\n",
      "accuracy_score\n",
      "0.8071636011616651\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = reporting.set_name(\"Test Classifier\").set_level('alc').create_report(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tom Work\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:976: DeprecationWarning: From version 0.18, binary input will not be handled specially when using averaged precision/recall/F-score. Please use average='binary' to report only the positive class performance.\n",
      "  'positive class performance.', DeprecationWarning)\n",
      "C:\\Users\\Tom Work\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:976: DeprecationWarning: From version 0.18, binary input will not be handled specially when using averaged precision/recall/F-score. Please use average='binary' to report only the positive class performance.\n",
      "  'positive class performance.', DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Results\n",
      "~~~~~~~~~~~~~~~~\n",
      "confusion_matrix\n",
      "[[2338, 3], [0, 4235]]\n",
      "\n",
      "\n",
      "classification_report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00      2341\n",
      "          1       1.00      1.00      1.00      4235\n",
      "\n",
      "avg / total       1.00      1.00      1.00      6576\n",
      "\n",
      "\n",
      "\n",
      "f1_score\n",
      "0.9996459341437508\n",
      "\n",
      "\n",
      "accuracy_score\n",
      "0.9995437956204379\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Testing Results Results\n",
      "~~~~~~~~~~~~~~~~~~~~~~~\n",
      "confusion_matrix\n",
      "[[448, 670], [313, 1809]]\n",
      "\n",
      "\n",
      "classification_report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.59      0.40      0.48      1118\n",
      "          1       0.73      0.85      0.79      2122\n",
      "\n",
      "avg / total       0.68      0.70      0.68      3240\n",
      "\n",
      "\n",
      "\n",
      "f1_score\n",
      "0.786350793305803\n",
      "\n",
      "\n",
      "accuracy_score\n",
      "0.696604938271605\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X, y = L.get_first_person()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.33, random_state=24)\n",
    "clf_fpa.fit(X_train, y_train)\n",
    "reporting = ClassificationReporting(clf_fpa, X_train, X_test, y_train, y_test, 2)\n",
    "report = reporting.set_name(\"Test Classifier\").set_level('fpa').create_report(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Results\n",
      "~~~~~~~~~~~~~~~~\n",
      "confusion_matrix\n",
      "[[2166, 0, 0], [0, 1131, 0], [0, 0, 962]]\n",
      "\n",
      "\n",
      "classification_report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00      2166\n",
      "          1       1.00      1.00      1.00      1131\n",
      "          2       1.00      1.00      1.00       962\n",
      "\n",
      "avg / total       1.00      1.00      1.00      4259\n",
      "\n",
      "\n",
      "\n",
      "f1_score\n",
      "1.0\n",
      "\n",
      "\n",
      "accuracy_score\n",
      "1.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Testing Results Results\n",
      "~~~~~~~~~~~~~~~~~~~~~~~\n",
      "confusion_matrix\n",
      "[[800, 172, 149], [139, 343, 63], [149, 62, 221]]\n",
      "\n",
      "\n",
      "classification_report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.74      0.71      0.72      1121\n",
      "          1       0.59      0.63      0.61       545\n",
      "          2       0.51      0.51      0.51       432\n",
      "\n",
      "avg / total       0.65      0.65      0.65      2098\n",
      "\n",
      "\n",
      "\n",
      "f1_score\n",
      "0.6510548553743395\n",
      "\n",
      "\n",
      "accuracy_score\n",
      "0.6501429933269781\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X, y = L.get_first_person_label()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.33, random_state=25)\n",
    "clf_fpl.fit(X_train, y_train)\n",
    "reporting = ClassificationReporting(clf_fpl, X_train, X_test, y_train, y_test, 3)\n",
    "report = reporting.set_name(\"Test Classifier\").set_level('fpl').create_report(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "folder = 'simple classifiers/'\n",
    "pickle.dump(clf_alc, open(folder +'clf_alc_simple.p', 'wb'))\n",
    "pickle.dump(clf_fpa, open(folder +'clf_fpa_simple.p', 'wb'))\n",
    "pickle.dump(clf_fpl, open(folder +'clf_fpl_simple.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "folder = 'simple classifiers/'\n",
    "clf_alc = pickle.load( open(folder +'clf_alc_simple.p', 'rb'))\n",
    "clf_fpa = pickle.load( open(folder +'clf_fpa_simple.p', 'rb'))\n",
    "clf_fpl = pickle.load( open(folder +'clf_fpl_simple.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('getter', ItemGetter(key='text')), ('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=200000, min_df=1,\n",
       "        ngram_range=(1, 3), norm='l2',...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_alc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from classification.prediction import PredictionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = PredictionTransformer(clf_alc, clf_fpa, clf_fpl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "folder = 'C:/Users/Tom Work/PycharmProjects/nyu-twipsy/tweets_split'\n",
    "all_tweets = pd.read_csv(folder + '/tweets_0.csv', encoding='utf8').dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "labeled= clf(all_tweets)\n",
    "pickle.dump(labeled, open('June_labeled_all_simple.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labeled.index = pd.to_datetime(labeled.created_at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(labeled, open('June_labeled_all_simple.p', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
