{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from data.turk import TurkResults2Label\n",
    "from data.dao import LabelGetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch1 = pd.read_csv('C:/users/tom work/downloads/Batch_2431673_batch_results.csv').drop_duplicates(subset='_id')\n",
    "batch2 = pd.read_csv('C:/users/tom work/downloads/Batch_2431727_batch_results.csv').drop_duplicates(subset='_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch1.index = batch1._id\n",
    "batch2.index = batch2._id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove workers that answered the same question twice..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "answers = batch1[batch1.Worker != batch2.Worker]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tom Work\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "answers['Answer2'] = batch2.ix[answers.index].Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Alcohol Related::Discussion',\n",
       " 'Alcohol Related::Promotional Content',\n",
       " 'First Person - Alcohol::Casual Drinking',\n",
       " 'First Person - Alcohol::Heavy Drinking',\n",
       " 'First Person - Alcohol::Looking to drink',\n",
       " 'First Person - Alcohol::Reflecting on drinking',\n",
       " 'Not Alcohol Related',\n",
       " '{}'}"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(answers.Answer.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of them have no answer {} ??????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels1= answers.Answer[answers.Answer != '{}'].apply(TurkResults2Label.parse_to_labels)\n",
    "labels2= answers.Answer2[answers.Answer2 != '{}'].apply(TurkResults2Label.parse_to_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(947,)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(973,)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch1['label1'] = labels1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch1['label2'] = labels2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labeled = batch1.dropna(subset=['label1', 'label2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "agreed = labeled[labeled.label1 == labeled.label2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(476, 9)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agreed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kappa Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alcohol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "L = LabelGetter(labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.57139053805809192"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xalc1, yalc1 = L._get_labels('alcohol', 'label1')\n",
    "Xalc2, yalc2 = L._get_labels('alcohol', 'label2')\n",
    "cohen_kappa_score(yalc1, yalc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_alc, y_alc = Xalc1[yalc1==yalc2], yalc1[yalc1==yalc2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(760, 9)"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_alc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2822212537559925"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L = LabelGetter(alc_agree)\n",
    "Xfpa1, yfpa1 = L._get_labels('first_person', 'label1')\n",
    "Xfpa2, yfpa2 = L._get_labels('first_person', 'label2')\n",
    "cohen_kappa_score(yfpa1, yfpa2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FPL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_fpa, y_fpa = Xfpa1[yfpa1==yfpa2], yfpa1[yfpa1==yfpa2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(330, 9)"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_fpa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35722100656455125"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L = LabelGetter(fpa_agree)\n",
    "Xfpl1, yfpl1 = L._get_labels('first_person_level', 'label1')\n",
    "Xfpl2, yfpl2 = L._get_labels('first_person_level', 'label2')\n",
    "cohen_kappa_score(yfpl1, yfpl2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_fpl, y_fpl = Xfpl1[yfpl1==yfpl2], yfpl1[yfpl1==yfpl2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(113, 9)"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_fpl.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like people are pretty good at deciding of a tweet is alcohol related, but FPA and FPL are more ambiguous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf_alc = pickle.load(open('pickles/clf_alc_UPDATED.p', 'rb'))\n",
    "clf_fpa = pickle.load(open('pickles/clf_fpa_UPDATED.p', 'rb'))\n",
    "clf_fpl = pickle.load(open('pickles/clf_fpl_double_labeled', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metrics = [accuracy_score, f1_score, confusion_matrix, classification_report, precision_score, recall_score]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_metrics(y_pred, y_true):\n",
    "    for metric in metrics:\n",
    "        kwargs = {}\n",
    "        if metric in [f1_score, precision_score, recall_score]:\n",
    "            kwargs[\"average\"] = \"weighted\"\n",
    "        print(metric.__name__, metric(y_true=y_true, y_pred=y_pred, **kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score 0.761842105263\n",
      "f1_score 0.829727187206\n",
      "confusion_matrix [[138 109]\n",
      " [ 72 441]]\n",
      "classification_report              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.66      0.56      0.60       247\n",
      "          1       0.80      0.86      0.83       513\n",
      "\n",
      "avg / total       0.75      0.76      0.76       760\n",
      "\n",
      "precision_score 0.801818181818\n",
      "recall_score 0.859649122807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tom Work\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:976: DeprecationWarning: From version 0.18, binary input will not be handled specially when using averaged precision/recall/F-score. Please use average='binary' to report only the positive class performance.\n",
      "  'positive class performance.', DeprecationWarning)\n",
      "C:\\Users\\Tom Work\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:976: DeprecationWarning: From version 0.18, binary input will not be handled specially when using averaged precision/recall/F-score. Please use average='binary' to report only the positive class performance.\n",
      "  'positive class performance.', DeprecationWarning)\n",
      "C:\\Users\\Tom Work\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:976: DeprecationWarning: From version 0.18, binary input will not be handled specially when using averaged precision/recall/F-score. Please use average='binary' to report only the positive class performance.\n",
      "  'positive class performance.', DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "print_metrics(clf_alc.predict(X_alc), y_alc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score 0.669696969697\n",
      "f1_score 0.764578833693\n",
      "confusion_matrix [[ 44  98]\n",
      " [ 11 177]]\n",
      "classification_report              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.31      0.45       142\n",
      "          1       0.64      0.94      0.76       188\n",
      "\n",
      "avg / total       0.71      0.67      0.63       330\n",
      "\n",
      "precision_score 0.643636363636\n",
      "recall_score 0.941489361702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tom Work\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:976: DeprecationWarning: From version 0.18, binary input will not be handled specially when using averaged precision/recall/F-score. Please use average='binary' to report only the positive class performance.\n",
      "  'positive class performance.', DeprecationWarning)\n",
      "C:\\Users\\Tom Work\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:976: DeprecationWarning: From version 0.18, binary input will not be handled specially when using averaged precision/recall/F-score. Please use average='binary' to report only the positive class performance.\n",
      "  'positive class performance.', DeprecationWarning)\n",
      "C:\\Users\\Tom Work\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:976: DeprecationWarning: From version 0.18, binary input will not be handled specially when using averaged precision/recall/F-score. Please use average='binary' to report only the positive class performance.\n",
      "  'positive class performance.', DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "print_metrics(clf_fpa.predict(X_fpa), y_fpa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score 0.610619469027\n",
      "f1_score 0.628726257946\n",
      "confusion_matrix [[36 12 14]\n",
      " [ 8 26  5]\n",
      " [ 3  2  7]]\n",
      "classification_report              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.77      0.58      0.66        62\n",
      "          1       0.65      0.67      0.66        39\n",
      "          2       0.27      0.58      0.37        12\n",
      "\n",
      "avg / total       0.67      0.61      0.63       113\n",
      "\n",
      "precision_score 0.673186999406\n",
      "recall_score 0.610619469027\n"
     ]
    }
   ],
   "source": [
    "print_metrics(clf_fpl.predict(X_fpl), y_fpl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
