{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from data.turk import TurkResults2Label\n",
    "from data.dao import LabelGetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch1 = pd.read_csv('C:/users/tom work/downloads/Batch_2431673_batch_results.csv').drop_duplicates(subset='_id')\n",
    "batch2 = pd.read_csv('C:/users/tom work/downloads/Batch_2431727_batch_results.csv').drop_duplicates(subset='_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch1.index = batch1._id\n",
    "batch2.index = batch2._id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove workers that answered the same question twice..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "answers = batch1[batch1.Worker != batch2.Worker]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tom Work\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "answers['Answer2'] = batch2.ix[answers.index].Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Alcohol Related::Discussion',\n",
       " 'Alcohol Related::Promotional Content',\n",
       " 'First Person - Alcohol::Casual Drinking',\n",
       " 'First Person - Alcohol::Heavy Drinking',\n",
       " 'First Person - Alcohol::Looking to drink',\n",
       " 'First Person - Alcohol::Reflecting on drinking',\n",
       " 'Not Alcohol Related',\n",
       " '{}'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(answers.Answer.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of them have no answer {} ??????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels1= answers.Answer[answers.Answer != '{}'].apply(TurkResults2Label.parse_to_labels)\n",
    "labels2= answers.Answer2[answers.Answer2 != '{}'].apply(TurkResults2Label.parse_to_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(947,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(973,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch1['label1'] = labels1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch1['label2'] = labels2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labeled = batch1.dropna(subset=['label1', 'label2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "agreed = labeled[labeled.label1 == labeled.label2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(476, 9)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agreed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kappa Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alcohol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "L = LabelGetter(labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.57139053805809192"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xalc1, yalc1 = L._get_labels('alcohol', 'label1')\n",
    "Xalc2, yalc2 = L._get_labels('alcohol', 'label2')\n",
    "cohen_kappa_score(yalc1, yalc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_alc, y_alc = Xalc1[yalc1==yalc2], yalc1[yalc1==yalc2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(760, 9)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_alc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2822212537559925"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L = LabelGetter(X_alc)\n",
    "Xfpa1, yfpa1 = L._get_labels('first_person', 'label1')\n",
    "Xfpa2, yfpa2 = L._get_labels('first_person', 'label2')\n",
    "cohen_kappa_score(yfpa1, yfpa2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FPL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_fpa, y_fpa = Xfpa1[yfpa1==yfpa2], yfpa1[yfpa1==yfpa2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(330, 9)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_fpa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35722100656455125"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L = LabelGetter(X_fpa)\n",
    "Xfpl1, yfpl1 = L._get_labels('first_person_level', 'label1')\n",
    "Xfpl2, yfpl2 = L._get_labels('first_person_level', 'label2')\n",
    "cohen_kappa_score(yfpl1, yfpl2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_fpl, y_fpl = Xfpl1[yfpl1==yfpl2], yfpl1[yfpl1==yfpl2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(113, 9)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_fpl.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like people are pretty good at deciding of a tweet is alcohol related, but FPA and FPL are more ambiguous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training set: June labeled data\n",
    "\n",
    "Test set: Sept labeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf_alc = pickle.load(open('pickles/clf_alc_UPDATED.p', 'rb'))\n",
    "clf_fpa = pickle.load(open('pickles/clf_fpa_UPDATED.p', 'rb'))\n",
    "clf_fpl = pickle.load(open('pickles/clf_fpl_double_labeled', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metrics = [accuracy_score, f1_score, confusion_matrix, classification_report]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_metrics(y_pred, y_true):\n",
    "    for metric in metrics:\n",
    "        kwargs = {}\n",
    "        if metric in [f1_score]:\n",
    "            kwargs[\"average\"] = \"weighted\"\n",
    "        print(metric.__name__ + ': \\n', metric(y_true=y_true, y_pred=y_pred, **kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score: \n",
      " 0.761842105263\n",
      "f1_score: \n",
      " 0.829727187206\n",
      "confusion_matrix: \n",
      " [[138 109]\n",
      " [ 72 441]]\n",
      "classification_report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.66      0.56      0.60       247\n",
      "          1       0.80      0.86      0.83       513\n",
      "\n",
      "avg / total       0.75      0.76      0.76       760\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tom Work\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:976: DeprecationWarning: From version 0.18, binary input will not be handled specially when using averaged precision/recall/F-score. Please use average='binary' to report only the positive class performance.\n",
      "  'positive class performance.', DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "print_metrics(clf_alc.predict(X_alc), y_alc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score: \n",
      " 0.669696969697\n",
      "f1_score: \n",
      " 0.764578833693\n",
      "confusion_matrix: \n",
      " [[ 44  98]\n",
      " [ 11 177]]\n",
      "classification_report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.80      0.31      0.45       142\n",
      "          1       0.64      0.94      0.76       188\n",
      "\n",
      "avg / total       0.71      0.67      0.63       330\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tom Work\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:976: DeprecationWarning: From version 0.18, binary input will not be handled specially when using averaged precision/recall/F-score. Please use average='binary' to report only the positive class performance.\n",
      "  'positive class performance.', DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "print_metrics(clf_fpa.predict(X_fpa), y_fpa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score: \n",
      " 0.610619469027\n",
      "f1_score: \n",
      " 0.628726257946\n",
      "confusion_matrix: \n",
      " [[36 12 14]\n",
      " [ 8 26  5]\n",
      " [ 3  2  7]]\n",
      "classification_report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.77      0.58      0.66        62\n",
      "          1       0.65      0.67      0.66        39\n",
      "          2       0.27      0.58      0.37        12\n",
      "\n",
      "avg / total       0.67      0.61      0.63       113\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_metrics(clf_fpl.predict(X_fpl), y_fpl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We want to see classifier performance on Sept training data vs June training data. Unfortunately, we don't have enough training data right now.\n",
    "\n",
    "Training set: 67% Sept labeled\n",
    "\n",
    "Test set: 33% Sept labeled\n",
    "\n",
    "Note: these sets are really tiny so it isn't very indicative of performance for the lower levels of hierarchy. These really should be performing better than the June training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score: \n",
      " 0.844621513944\n",
      "f1_score: \n",
      " 0.893150684932\n",
      "confusion_matrix: \n",
      " [[ 49  25]\n",
      " [ 14 163]]\n",
      "classification_report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.78      0.66      0.72        74\n",
      "          1       0.87      0.92      0.89       177\n",
      "\n",
      "avg / total       0.84      0.84      0.84       251\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tom Work\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:976: DeprecationWarning: From version 0.18, binary input will not be handled specially when using averaged precision/recall/F-score. Please use average='binary' to report only the positive class performance.\n",
      "  'positive class performance.', DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_alc, y_alc, test_size=0.33, random_state=26)\n",
    "clf_alc.fit(X_train, y_train)\n",
    "print_metrics(clf_alc.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score: \n",
      " 0.633027522936\n",
      "f1_score: \n",
      " 0.692307692308\n",
      "confusion_matrix: \n",
      " [[24 22]\n",
      " [18 45]]\n",
      "classification_report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.57      0.52      0.55        46\n",
      "          1       0.67      0.71      0.69        63\n",
      "\n",
      "avg / total       0.63      0.63      0.63       109\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tom Work\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:976: DeprecationWarning: From version 0.18, binary input will not be handled specially when using averaged precision/recall/F-score. Please use average='binary' to report only the positive class performance.\n",
      "  'positive class performance.', DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_fpa, y_fpa, test_size=0.33, random_state=26)\n",
    "clf_fpa.fit(X_train, y_train)\n",
    "print_metrics(clf_fpa.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score: \n",
      " 0.526315789474\n",
      "f1_score: \n",
      " 0.427848467322\n",
      "confusion_matrix: \n",
      " [[17  0  0]\n",
      " [15  3  0]\n",
      " [ 3  0  0]]\n",
      "classification_report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.49      1.00      0.65        17\n",
      "          1       1.00      0.17      0.29        18\n",
      "          2       0.00      0.00      0.00         3\n",
      "\n",
      "avg / total       0.69      0.53      0.43        38\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tom Work\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Tom Work\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1074: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_fpl, y_fpl, test_size=0.33, random_state=20)\n",
    "clf_fpl.fit(X_train, y_train)\n",
    "print_metrics(clf_fpl.predict(X_test), y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
